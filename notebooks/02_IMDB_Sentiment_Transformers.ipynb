{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMDB Sentiment Analysis** - Binary Classification\n",
    "- **Date**: Mar 8, 2024  \n",
    "- **Task**: Create a model to classify reviews into positive or negative using the attention mechanism \n",
    "- **Procedure**: Analyze data with pandas, create nn model in TensorFlow, implement transformers\n",
    "- **Dataset source**: https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format/data   \n",
    "- **References**: https://github.com/PhilChodrow/PIC16B/blob/7d12d32e070e7ff3840b971c0ce4185ef1911796/discussion/tmdb.ipynb#L758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load libraries and custom functions\n",
    "# Matrices and datasets ------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Graphics -------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Text processors\n",
    "import re\n",
    "import string\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "# Machine Learning -----------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Deep Learning --------------------------------------------------------\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Custom functions -----------------------------------------------------\n",
    "def sentence_fixed_split(x:list, words: int):\n",
    "    \"\"\"\n",
    "    Split a list of sentences into a list of fixed length sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: list\n",
    "        sentence as a list of words\n",
    "    words: int \n",
    "        number of fixed words required\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of fixed length sentences\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        df = pd.DataFrame({'text':['In our darkest hour, I will prevail as always']})\n",
    "        df['text'] = df['text'].apply(lambda x: sentence_fixed_split(x, 4))\n",
    "        df = df.explode('text')\n",
    "        text\n",
    "        ----\n",
    "        In our darkest hour,\n",
    "        I will prevail as\n",
    "        always\n",
    "    \"\"\"\n",
    "    words_lenght = len(x.split(' '))\n",
    "    if words_lenght>1 and words > 1 and words_lenght > words:\n",
    "        return [' '.join(x.split(' ')[i:i+words]) for i in range(0, len(x.split(' ')), words)]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def plot_accuracy_loss_tfmodel(model, epochs: int):\n",
    "    '''\n",
    "    Plots the accuracy and loss curves of a TensorFlow model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        A tensorflow model\n",
    "    epochs\n",
    "        Number of epochs the model was trained for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A 2 columns 1 row plot of accuracy and loss curves\n",
    "    '''\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, model.history['accuracy'], 'b', label='Training accuracy')\n",
    "    plt.plot(epochs_range, model.history['val_accuracy'], 'b--', label='Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, model.history['loss'], 'b', label='Training loss')\n",
    "    plt.plot(epochs_range, model.history['val_loss'], 'b--', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Plots the confusion matrix and precision/recall metrics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True labels\n",
    "    y_pred\n",
    "        Predicted labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A plot and the metrics\n",
    "    '''\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    plt.close\n",
    "    print(classification_report(y_pred, y_true))\n",
    "\n",
    "def analyze_wrong_predictions(dataset, y_pred, samples):\n",
    "    '''\n",
    "    Prints samples of wrong predictions on a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        data with values\n",
    "    y_pred\n",
    "        list of predictions\n",
    "    samples\n",
    "        number of samples required\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Text with true label and reviews\n",
    "    '''\n",
    "    dataset['prediction'] = y_pred\n",
    "    for index, row in dataset[dataset.label != dataset.prediction].sample(samples).iterrows():\n",
    "        print(f'label: {row.label}, {row.text}')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    40000 non-null  object\n",
      " 1   label   40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Load data\n",
    "# 1.1 Read csv and get basic info\n",
    "df_train = pd.read_csv('../data/01_IMDB_Train.csv')\n",
    "df_val = pd.read_csv('../data/01_IMDB_Valid.csv')\n",
    "df_test = pd.read_csv('../data/01_IMDB_Test.csv')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Preprocess data based on observed information\n",
    "df_train = df_train[~df_train.text.duplicated()]\n",
    "df_val = df_val[~df_val.text.duplicated()]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:13:56.005063: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-03-09 22:13:56.005088: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-03-09 22:13:56.005092: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-03-09 22:13:56.005128: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-09 22:13:56.005145: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Create a neural network using transformers\n",
    "# 2.1 Create tensorflow dataset\n",
    "def make_data(dataset):\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {'text':dataset['text']},\n",
    "            dataset['label']\n",
    "        )\n",
    "    )\n",
    "train = make_data(df_train).batch(32)\n",
    "val = make_data(df_val).batch(32)\n",
    "test = make_data(df_test).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Create functions to process text on TF string tensors\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    '''\n",
    "    This function does remove blank spaces, lowercases the text, removes \n",
    "    html tags, reduces whitespaces, sets spaces after comma, replaces \n",
    "    tripple representations of vowels and removes punctuations \n",
    "    '''\n",
    "    stripped_string = tf.strings.strip(string_tensor)\n",
    "    lowercase_string = tf.strings.lower(stripped_string)\n",
    "    tagless_string = tf.strings.regex_replace(lowercase_string, '<.*?>', '')\n",
    "    single_space_string = tf.strings.regex_replace(tagless_string, '\\s+',' ')\n",
    "    comma_space_string = tf.strings.regex_replace(single_space_string, '\\s,\\s',', ')\n",
    "    simple_vowel_a_string = tf.strings.regex_replace(comma_space_string, 'a{3,}', 'a')\n",
    "    simple_vowel_e_string = tf.strings.regex_replace(simple_vowel_a_string, 'e{3,}', 'e')\n",
    "    simple_vowel_i_string = tf.strings.regex_replace(simple_vowel_e_string, 'i{3, }', 'i')\n",
    "    simple_vowel_o_string = tf.strings.regex_replace(simple_vowel_i_string, 'o{3, }', 'o')\n",
    "    simple_vowel_u_string = tf.strings.regex_replace(simple_vowel_o_string, 'u{3, }', 'u')\n",
    "    stripped_string_again = tf.strings.strip(simple_vowel_u_string)\n",
    "    return tf.strings.regex_replace(stripped_string_again,\n",
    "                                    f\"[{re.escape(string.punctuation)}]\",'')\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    '''\n",
    "    This function splits the string tensor\n",
    "    '''\n",
    "    return tf.strings.split(string_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:13:56.143882: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Create the vectorization layer and function adapted to train\n",
    "def create_vectorize_layer(train, feature):\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization_fn,\n",
    "        split=custom_split_fn,\n",
    "        max_tokens=20000,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=20000\n",
    "    )\n",
    "    vectorize_layer.adapt(train.map(lambda x, y: x[feature]))\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_text = create_vectorize_layer(train, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs.shape (32, 20000)\n",
      "Targets.shape (32,)\n",
      "Inputs[0] tf.Tensor([  10 2090   55 ...    0    0    0], shape=(20000,), dtype=int64)\n",
      "Targets[0] tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Expand the tf datasets applying vectorization\n",
    "int_train_ds = train.map(\n",
    "    lambda x,y: (vectorize_text(x['text']), y), \n",
    "    num_parallel_calls=4)\n",
    "\n",
    "int_val_ds = val.map(\n",
    "    lambda x,y: (vectorize_text(x['text']), y), \n",
    "    num_parallel_calls=4)\n",
    "\n",
    "int_test_ds = test.map(\n",
    "    lambda x,y: (vectorize_text(x['text']), y), \n",
    "    num_parallel_calls=4)\n",
    "\n",
    "for inputs, targets in int_train_ds:\n",
    "    print('Inputs.shape', inputs.shape)\n",
    "    print('Targets.shape', targets.shape)\n",
    "    print('Inputs[0]', inputs[0])\n",
    "    print('Targets[0]', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create transformer layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dense_dim': self.dense_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, None, 256)         1069600   \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6189857 (23.61 MB)\n",
      "Trainable params: 6189857 (23.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "x =layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('02_model_transformer.keras',\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, epochs=20, validation_data=int_val_ds, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
