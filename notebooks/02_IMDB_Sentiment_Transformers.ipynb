{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMDB Sentiment Analysis** - Binary Classification\n",
    "- **Date**: Mar 8, 2024  \n",
    "- **Task**: Create a model to classify reviews into positive or negative using the attention mechanism \n",
    "- **Procedure**: Analyze data with pandas, create nn model in TensorFlow, implement transformers\n",
    "- **Dataset source**: https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format/data   \n",
    "- **References**: https://github.com/PhilChodrow/PIC16B/blob/7d12d32e070e7ff3840b971c0ce4185ef1911796/discussion/tmdb.ipynb#L758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load libraries and custom functions\n",
    "# Matrices and datasets ------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Graphics -------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Text processors\n",
    "import re\n",
    "import string\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "# Machine Learning -----------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Deep Learning --------------------------------------------------------\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Custom functions -----------------------------------------------------\n",
    "def sentence_fixed_split(x:list, words: int):\n",
    "    \"\"\"\n",
    "    Split a list of sentences into a list of fixed length sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: list\n",
    "        sentence as a list of words\n",
    "    words: int \n",
    "        number of fixed words required\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of fixed length sentences\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        df = pd.DataFrame({'text':['In our darkest hour, I will prevail as always']})\n",
    "        df['text'] = df['text'].apply(lambda x: sentence_fixed_split(x, 4))\n",
    "        df = df.explode('text')\n",
    "        text\n",
    "        ----\n",
    "        In our darkest hour,\n",
    "        I will prevail as\n",
    "        always\n",
    "    \"\"\"\n",
    "    words_lenght = len(x.split(' '))\n",
    "    if words_lenght>1 and words > 1 and words_lenght > words:\n",
    "        return [' '.join(x.split(' ')[i:i+words]) for i in range(0, len(x.split(' ')), words)]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def plot_accuracy_loss_tfmodel(model, epochs: int):\n",
    "    '''\n",
    "    Plots the accuracy and loss curves of a TensorFlow model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        A tensorflow model\n",
    "    epochs\n",
    "        Number of epochs the model was trained for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A 2 columns 1 row plot of accuracy and loss curves\n",
    "    '''\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, model.history['accuracy'], 'b', label='Training accuracy')\n",
    "    plt.plot(epochs_range, model.history['val_accuracy'], 'b--', label='Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, model.history['loss'], 'b', label='Training loss')\n",
    "    plt.plot(epochs_range, model.history['val_loss'], 'b--', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Plots the confusion matrix and precision/recall metrics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True labels\n",
    "    y_pred\n",
    "        Predicted labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A plot and the metrics\n",
    "    '''\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    plt.close\n",
    "    print(classification_report(y_pred, y_true))\n",
    "\n",
    "def analyze_wrong_predictions(dataset, y_pred, samples):\n",
    "    '''\n",
    "    Prints samples of wrong predictions on a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        data with values\n",
    "    y_pred\n",
    "        list of predictions\n",
    "    samples\n",
    "        number of samples required\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Text with true label and reviews\n",
    "    '''\n",
    "    dataset['prediction'] = y_pred\n",
    "    for index, row in dataset[dataset.label != dataset.prediction].sample(samples).iterrows():\n",
    "        print(f'label: {row.label}, {row.text}')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    40000 non-null  object\n",
      " 1   label   40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Load data\n",
    "# 1.1 Read csv and get basic info\n",
    "df_train = pd.read_csv('../data/01_IMDB_Train.csv')\n",
    "df_val = pd.read_csv('../data/01_IMDB_Valid.csv')\n",
    "df_test = pd.read_csv('../data/01_IMDB_Test.csv')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Preprocess data based on observed information\n",
    "df_train = df_train[~df_train.text.duplicated()]\n",
    "df_val = df_val[~df_val.text.duplicated()]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 10:48:10.489953: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-03-09 10:48:10.489985: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-03-09 10:48:10.490012: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-03-09 10:48:10.490300: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-09 10:48:10.490680: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Create a neural network using transformers\n",
    "# 2.1 Create tensorflow dataset\n",
    "def make_data(dataset):\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {'text':dataset['text']},\n",
    "            dataset['label']\n",
    "        )\n",
    "    )\n",
    "train = make_data(df_train).batch(32)\n",
    "val = make_data(df_val).batch(32)\n",
    "test = make_data(df_test).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create transformer layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_head=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dense_dim': self.dense_dim\n",
    "        })\n",
    "        return config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
