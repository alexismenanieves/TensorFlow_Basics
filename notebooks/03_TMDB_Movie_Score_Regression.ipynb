{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TMDB Score Prediction** - Regression with deep learning\n",
    "- **Date**: Mar 6, 2024  \n",
    "- **Task**: Create a model to predict movie score based on text and numeric inputs \n",
    "- **Procedure**: Analyze data with pandas, create nn model in TensorFlow\n",
    "- **Dataset source**: https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata/data \n",
    "- **References**: https://github.com/PhilChodrow/PIC16B/blob/7d12d32e070e7ff3840b971c0ce4185ef1911796/discussion/tmdb.ipynb#L758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load libraries and custom functions\n",
    "# Matrices and datasets ------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Graphics -------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Text processors\n",
    "import re\n",
    "import string\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "# Machine Learning -----------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Deep Learning --------------------------------------------------------\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4803 entries, 0 to 4802\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   budget                4803 non-null   int64  \n",
      " 1   genres                4803 non-null   object \n",
      " 2   homepage              1712 non-null   object \n",
      " 3   id                    4803 non-null   int64  \n",
      " 4   keywords              4803 non-null   object \n",
      " 5   original_language     4803 non-null   object \n",
      " 6   original_title        4803 non-null   object \n",
      " 7   overview              4800 non-null   object \n",
      " 8   popularity            4803 non-null   float64\n",
      " 9   production_companies  4803 non-null   object \n",
      " 10  production_countries  4803 non-null   object \n",
      " 11  release_date          4802 non-null   object \n",
      " 12  revenue               4803 non-null   int64  \n",
      " 13  runtime               4801 non-null   float64\n",
      " 14  spoken_languages      4803 non-null   object \n",
      " 15  status                4803 non-null   object \n",
      " 16  tagline               3959 non-null   object \n",
      " 17  title                 4803 non-null   object \n",
      " 18  vote_average          4803 non-null   float64\n",
      " 19  vote_count            4803 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(13)\n",
      "memory usage: 750.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Load data\n",
    "# 1.1 Read csv and get basic info\n",
    "df_raw = pd.read_csv('../data/02_TMDB_5000_movies.csv')\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>0</td>\n",
       "      <td>[{\"id\": 16, \"name\": \"Animation\"}, {\"id\": 10751...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13682</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>Pooh's Heffalump Movie</td>\n",
       "      <td>Who or what exactly is a Heffalump? The lovabl...</td>\n",
       "      <td>9.031540</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}]</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2005-02-11</td>\n",
       "      <td>0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>There's something new in the Hundred Acre Wood.</td>\n",
       "      <td>Pooh's Heffalump Movie</td>\n",
       "      <td>6.4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>8000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 53, \"nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13154</td>\n",
       "      <td>[{\"id\": 1794, \"name\": \"yakuza\"}, {\"id\": 12670,...</td>\n",
       "      <td>en</td>\n",
       "      <td>Showdown in Little Tokyo</td>\n",
       "      <td>An American with a Japanese upbringing, Chris ...</td>\n",
       "      <td>8.403859</td>\n",
       "      <td>[{\"name\": \"Original Pictures\", \"id\": 4234}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>1991-08-23</td>\n",
       "      <td>2275557</td>\n",
       "      <td>79.0</td>\n",
       "      <td>[{\"iso_639_1\": \"ja\", \"name\": \"\\u65e5\\u672c\\u8a...</td>\n",
       "      <td>Released</td>\n",
       "      <td>One's a warrior. One's a wise guy. They're two...</td>\n",
       "      <td>Showdown in Little Tokyo</td>\n",
       "      <td>5.7</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>49000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 35, \"nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9548</td>\n",
       "      <td>[{\"id\": 578, \"name\": \"rock and roll\"}, {\"id\": ...</td>\n",
       "      <td>en</td>\n",
       "      <td>The Adventures of Ford Fairlane</td>\n",
       "      <td>Ford \"Mr. Rock n' Roll Detective\" Fairlane is ...</td>\n",
       "      <td>2.808428</td>\n",
       "      <td>[{\"name\": \"Twentieth Century Fox Film Corporat...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>1990-07-11</td>\n",
       "      <td>20423389</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Kojak. Columbo. Dirty Harry. Wimps.</td>\n",
       "      <td>The Adventures of Ford Fairlane</td>\n",
       "      <td>6.2</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>32000000</td>\n",
       "      <td>[{\"id\": 18, \"name\": \"Drama\"}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13920</td>\n",
       "      <td>[{\"id\": 5565, \"name\": \"biography\"}, {\"id\": 605...</td>\n",
       "      <td>en</td>\n",
       "      <td>Radio</td>\n",
       "      <td>High school football coach, Harold Jones befri...</td>\n",
       "      <td>9.254647</td>\n",
       "      <td>[{\"name\": \"Revolution Studios\", \"id\": 497}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2003-10-24</td>\n",
       "      <td>52277485</td>\n",
       "      <td>109.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>His courage made them champions.</td>\n",
       "      <td>Radio</td>\n",
       "      <td>6.8</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>18339750</td>\n",
       "      <td>[{\"id\": 18, \"name\": \"Drama\"}, {\"id\": 36, \"name...</td>\n",
       "      <td>http://www.downfallthefilm.com/</td>\n",
       "      <td>613</td>\n",
       "      <td>[{\"id\": 220, \"name\": \"berlin\"}, {\"id\": 351, \"n...</td>\n",
       "      <td>de</td>\n",
       "      <td>Der Untergang</td>\n",
       "      <td>In April of 1945, Germany stands at the brink ...</td>\n",
       "      <td>32.445895</td>\n",
       "      <td>[{\"name\": \"Degeto Film\", \"id\": 986}, {\"name\": ...</td>\n",
       "      <td>[{\"iso_3166_1\": \"AT\", \"name\": \"Austria\"}, {\"is...</td>\n",
       "      <td>2004-09-08</td>\n",
       "      <td>92180910</td>\n",
       "      <td>156.0</td>\n",
       "      <td>[{\"iso_639_1\": \"hu\", \"name\": \"Magyar\"}, {\"iso_...</td>\n",
       "      <td>Released</td>\n",
       "      <td>April 1945, a nation awaits its...Downfall</td>\n",
       "      <td>Downfall</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        budget                                             genres  \\\n",
       "2182         0  [{\"id\": 16, \"name\": \"Animation\"}, {\"id\": 10751...   \n",
       "3274   8000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 53, \"nam...   \n",
       "1003  49000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 35, \"nam...   \n",
       "1383  32000000                      [{\"id\": 18, \"name\": \"Drama\"}]   \n",
       "2724  18339750  [{\"id\": 18, \"name\": \"Drama\"}, {\"id\": 36, \"name...   \n",
       "\n",
       "                             homepage     id  \\\n",
       "2182                              NaN  13682   \n",
       "3274                              NaN  13154   \n",
       "1003                              NaN   9548   \n",
       "1383                              NaN  13920   \n",
       "2724  http://www.downfallthefilm.com/    613   \n",
       "\n",
       "                                               keywords original_language  \\\n",
       "2182                                                 []                en   \n",
       "3274  [{\"id\": 1794, \"name\": \"yakuza\"}, {\"id\": 12670,...                en   \n",
       "1003  [{\"id\": 578, \"name\": \"rock and roll\"}, {\"id\": ...                en   \n",
       "1383  [{\"id\": 5565, \"name\": \"biography\"}, {\"id\": 605...                en   \n",
       "2724  [{\"id\": 220, \"name\": \"berlin\"}, {\"id\": 351, \"n...                de   \n",
       "\n",
       "                       original_title  \\\n",
       "2182           Pooh's Heffalump Movie   \n",
       "3274         Showdown in Little Tokyo   \n",
       "1003  The Adventures of Ford Fairlane   \n",
       "1383                            Radio   \n",
       "2724                    Der Untergang   \n",
       "\n",
       "                                               overview  popularity  \\\n",
       "2182  Who or what exactly is a Heffalump? The lovabl...    9.031540   \n",
       "3274  An American with a Japanese upbringing, Chris ...    8.403859   \n",
       "1003  Ford \"Mr. Rock n' Roll Detective\" Fairlane is ...    2.808428   \n",
       "1383  High school football coach, Harold Jones befri...    9.254647   \n",
       "2724  In April of 1945, Germany stands at the brink ...   32.445895   \n",
       "\n",
       "                                   production_companies  \\\n",
       "2182        [{\"name\": \"Walt Disney Pictures\", \"id\": 2}]   \n",
       "3274  [{\"name\": \"Original Pictures\", \"id\": 4234}, {\"...   \n",
       "1003  [{\"name\": \"Twentieth Century Fox Film Corporat...   \n",
       "1383  [{\"name\": \"Revolution Studios\", \"id\": 497}, {\"...   \n",
       "2724  [{\"name\": \"Degeto Film\", \"id\": 986}, {\"name\": ...   \n",
       "\n",
       "                                   production_countries release_date  \\\n",
       "2182  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2005-02-11   \n",
       "3274  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   1991-08-23   \n",
       "1003  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   1990-07-11   \n",
       "1383  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2003-10-24   \n",
       "2724  [{\"iso_3166_1\": \"AT\", \"name\": \"Austria\"}, {\"is...   2004-09-08   \n",
       "\n",
       "       revenue  runtime                                   spoken_languages  \\\n",
       "2182         0     68.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]   \n",
       "3274   2275557     79.0  [{\"iso_639_1\": \"ja\", \"name\": \"\\u65e5\\u672c\\u8a...   \n",
       "1003  20423389    104.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]   \n",
       "1383  52277485    109.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]   \n",
       "2724  92180910    156.0  [{\"iso_639_1\": \"hu\", \"name\": \"Magyar\"}, {\"iso_...   \n",
       "\n",
       "        status                                            tagline  \\\n",
       "2182  Released    There's something new in the Hundred Acre Wood.   \n",
       "3274  Released  One's a warrior. One's a wise guy. They're two...   \n",
       "1003  Released                Kojak. Columbo. Dirty Harry. Wimps.   \n",
       "1383  Released                   His courage made them champions.   \n",
       "2724  Released         April 1945, a nation awaits its...Downfall   \n",
       "\n",
       "                                title  vote_average  vote_count  \n",
       "2182           Pooh's Heffalump Movie           6.4          88  \n",
       "3274         Showdown in Little Tokyo           5.7          95  \n",
       "1003  The Adventures of Ford Fairlane           6.2          71  \n",
       "1383                            Radio           6.8         141  \n",
       "2724                         Downfall           7.7        1037  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 Get a sample\n",
    "df_raw.sample(5, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns contains nested json data, other contains unique \n",
    "information like ids or names, so let's transform our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess data\n",
    "# 2.1 Create a interim dataset for transformations, drop unused columns and NAs\n",
    "df_interim = df_raw.copy()\n",
    "df_interim = df_interim.drop(columns=['id','original_title','title','vote_count','original_language','homepage'])\n",
    "df_interim = df_interim.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can concatenate each id value in json format to form a id collection\n",
    "and process it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Concatenate strings in json format and drop changed columns\n",
    "df_interim['genres_c'] = df_interim['genres'].apply(lambda x: ' '.join([str(y['id']) for y in eval(x)]))\n",
    "df_interim['keywords_c'] = df_interim['keywords'].apply(lambda x: ' '.join([str(y['id']) for y in eval(x)]))\n",
    "df_interim['producers_c'] = df_interim['production_companies'].apply(lambda x: ' '.join([str(y['id']) for y in eval(x)]))\n",
    "df_interim['countries_c'] = df_interim['production_countries'].apply(lambda x: ' '.join([str(y['iso_3166_1']) for y in eval(x)]))\n",
    "df_interim['languages_c'] = df_interim['spoken_languages'].apply(lambda x: ' '.join([str(y['iso_639_1']) for y in eval(x)]))\n",
    "df_interim = df_interim.drop(columns=['genres','keywords','production_companies','production_countries','spoken_languages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>genres_c</th>\n",
       "      <th>keywords_c</th>\n",
       "      <th>producers_c</th>\n",
       "      <th>countries_c</th>\n",
       "      <th>languages_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>20000000</td>\n",
       "      <td>Oscar and Peter land a career-making opportuni...</td>\n",
       "      <td>7.553773</td>\n",
       "      <td>1999-10-22</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>All's fair in the war of love.</td>\n",
       "      <td>5.4</td>\n",
       "      <td>35 10749</td>\n",
       "      <td>237 1691 2301 2679 2683 4480 5265 6281 6351 97...</td>\n",
       "      <td>79 6194</td>\n",
       "      <td>AU US</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>25500000</td>\n",
       "      <td>Two brothers, on either side of the law, face ...</td>\n",
       "      <td>12.756344</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>2415472</td>\n",
       "      <td>128.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>Crime runs in the family.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>53 80 18</td>\n",
       "      <td></td>\n",
       "      <td>856 2490 2612 2908 5358 7454 9015 10611 11261 ...</td>\n",
       "      <td>FR US</td>\n",
       "      <td>es en it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        budget                                           overview  popularity  \\\n",
       "2214  20000000  Oscar and Peter land a career-making opportuni...    7.553773   \n",
       "1873  25500000  Two brothers, on either side of the law, face ...   12.756344   \n",
       "\n",
       "     release_date  revenue  runtime    status                         tagline  \\\n",
       "2214   1999-10-22        0     98.0  Released  All's fair in the war of love.   \n",
       "1873   2013-08-22  2415472    128.0  Released       Crime runs in the family.   \n",
       "\n",
       "      vote_average  genres_c  \\\n",
       "2214           5.4  35 10749   \n",
       "1873           6.0  53 80 18   \n",
       "\n",
       "                                             keywords_c  \\\n",
       "2214  237 1691 2301 2679 2683 4480 5265 6281 6351 97...   \n",
       "1873                                                      \n",
       "\n",
       "                                            producers_c countries_c  \\\n",
       "2214                                            79 6194       AU US   \n",
       "1873  856 2490 2612 2908 5358 7454 9015 10611 11261 ...       FR US   \n",
       "\n",
       "     languages_c  \n",
       "2214          en  \n",
       "1873    es en it  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3959, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3 View results and current shape\n",
    "display(df_interim.sample(2))\n",
    "df_interim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some information comes as numeric like budget. But since values in \n",
    "budgets are quite large, we can apply some transformations like log. To \n",
    "avoid zeros, we can add 1 to all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Transform scale in numeric variables\n",
    "df_interim['budget_log'] = np.log(df_interim['budget']+1)\n",
    "df_interim['revenue_log'] = np.log(df_interim['revenue']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the date, we can use a point of reference, like the year of the \n",
    "oldest movie as starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Transform the date\n",
    "df_interim['Year_t'] = df_interim['release_date'].apply(lambda x: float(str(x)[0:4]) if (str(x)[0:4])!='' else 2000)\n",
    "df_interim['Month_t'] = df_interim['release_date'].apply(lambda x: float(str(x)[5:7]) if (str(x)[5:7])!='' else 1)\n",
    "df_interim = df_interim.drop(columns=['release_date'])\n",
    "df_interim['Year_diff'] = df_interim['Year_t'] - min(df_interim['Year_t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>genres_c</th>\n",
       "      <th>keywords_c</th>\n",
       "      <th>producers_c</th>\n",
       "      <th>countries_c</th>\n",
       "      <th>languages_c</th>\n",
       "      <th>budget_log</th>\n",
       "      <th>revenue_log</th>\n",
       "      <th>Year_t</th>\n",
       "      <th>Month_t</th>\n",
       "      <th>Year_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>John Matrix, the former leader of a special co...</td>\n",
       "      <td>34.224204</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>Somewhere... somehow... someone's going to pay.</td>\n",
       "      <td>6.4</td>\n",
       "      <td>28 12 53</td>\n",
       "      <td>1930 3568 5600 5905 11107</td>\n",
       "      <td>306 396 1885</td>\n",
       "      <td>US</td>\n",
       "      <td>en</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>17.867296</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>A horror comedy based on the ancient legend ab...</td>\n",
       "      <td>31.565117</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>You don't want to be on his list.</td>\n",
       "      <td>5.9</td>\n",
       "      <td>27 35 14</td>\n",
       "      <td>657 1442 1991 3373 5570 10794 11183 14755 1479...</td>\n",
       "      <td>33 923</td>\n",
       "      <td>US</td>\n",
       "      <td>de en</td>\n",
       "      <td>16.523561</td>\n",
       "      <td>17.935339</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>Musical adaptation of Charles Dickens' Oliver ...</td>\n",
       "      <td>8.305998</td>\n",
       "      <td>153.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>Much Much More Than a Musical!</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18 10751 10402</td>\n",
       "      <td>3430 4344 8250 13014</td>\n",
       "      <td>441 1807 3632</td>\n",
       "      <td>GB</td>\n",
       "      <td>en</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>17.437258</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               overview  popularity  runtime  \\\n",
       "2996  John Matrix, the former leader of a special co...   34.224204     90.0   \n",
       "2540  A horror comedy based on the ancient legend ab...   31.565117     98.0   \n",
       "2992  Musical adaptation of Charles Dickens' Oliver ...    8.305998    153.0   \n",
       "\n",
       "        status                                          tagline  vote_average  \\\n",
       "2996  Released  Somewhere... somehow... someone's going to pay.           6.4   \n",
       "2540  Released                You don't want to be on his list.           5.9   \n",
       "2992  Released                   Much Much More Than a Musical!           7.0   \n",
       "\n",
       "            genres_c                                         keywords_c  \\\n",
       "2996        28 12 53                          1930 3568 5600 5905 11107   \n",
       "2540        27 35 14  657 1442 1991 3373 5570 10794 11183 14755 1479...   \n",
       "2992  18 10751 10402                               3430 4344 8250 13014   \n",
       "\n",
       "        producers_c countries_c languages_c  budget_log  revenue_log  Year_t  \\\n",
       "2996   306 396 1885          US          en   16.118096    17.867296  1985.0   \n",
       "2540         33 923          US       de en   16.523561    17.935339  2015.0   \n",
       "2992  441 1807 3632          GB          en   16.118096    17.437258  1968.0   \n",
       "\n",
       "      Month_t  Year_diff  \n",
       "2996     10.0       69.0  \n",
       "2540     11.0       99.0  \n",
       "2992      9.0       52.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3959, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.6 Create final dataset\n",
    "df = df_interim.drop(['budget','revenue'], axis=1).copy()\n",
    "display(df.sample(3, random_state=2024))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create the model and train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions in train: (2533, 15), validation: (634, 15) and test: (792, 15)\n"
     ]
    }
   ],
   "source": [
    "# Step 3. Create the model based on the dataset\n",
    "# 3.1 Split the dataset into training and testing sets\n",
    "X =  df.drop(columns='vote_average').copy()\n",
    "y = df['vote_average'].copy()/10\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2024)\n",
    "print(f'Dimensions in train: {X_train.shape}, validation: {X_val.shape} and test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use tensorflow datasets, with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 19:01:23.562915: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-03-06 19:01:23.562937: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-03-06 19:01:23.562943: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-03-06 19:01:23.562974: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-06 19:01:23.562989: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Create tensorflow datasets with batches\n",
    "def make_data(X,y):\n",
    "    # Be careful with nan values, they are not supported by tf.data.Dataset.from_tensor_slices\n",
    "    # And don't forget there are two parentheses!!!!\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "        {\n",
    "            'genres_c': X['genres_c'],\n",
    "            'keywords_c': X['keywords_c'],\n",
    "            'overview': X['overview'],\n",
    "            'producers_c': X['producers_c'],\n",
    "            'countries_c': X['countries_c'],\n",
    "            'languages_c': X['languages_c'],\n",
    "            'tagline': X['tagline'],\n",
    "            'scalars': X[['budget_log','revenue_log','popularity','runtime','Year_diff','Month_t']]\n",
    "        },\n",
    "        {\n",
    "            'vote_average':y\n",
    "        }\n",
    "        )\n",
    "    )\n",
    "train = make_data(X_train, y_train).batch(20)\n",
    "val = make_data(X_val, y_val).batch(20)\n",
    "test = make_data(X_test, y_test).batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Create support functions for text vectorization\n",
    "def standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
    "    stripped = tf.strings.strip(no_punctuation)\n",
    "    return stripped\n",
    "\n",
    "def create_vectorized_layer(train, feature):\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=standardization,\n",
    "        max_tokens=2000,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=500)\n",
    "    vectorize_layer.adapt(train.map(lambda x,y: x[feature]))\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 19:01:23.681954: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Vectorize text fields\n",
    "vectorized_genres = create_vectorized_layer(train, 'genres_c')\n",
    "vectorized_keywords = create_vectorized_layer(train, 'keywords_c')\n",
    "vectorized_overview = create_vectorized_layer(train, 'overview')\n",
    "vectorized_producers = create_vectorized_layer(train, 'producers_c')\n",
    "vectorized_countries = create_vectorized_layer(train, 'countries_c')\n",
    "vectorized_languages = create_vectorized_layer(train, 'languages_c')\n",
    "vectorized_tagline = create_vectorized_layer(train, 'tagline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Create support functions for inputs\n",
    "def create_string_input(name):\n",
    "    return keras.Input(\n",
    "        shape=(1,),\n",
    "        name = name,\n",
    "        dtype = 'string'\n",
    "    ) \n",
    "\n",
    "def create_numeric_input(name):\n",
    "    return keras.Input(\n",
    "        shape=(6,),\n",
    "        name=\"scalars\",\n",
    "        dtype=\"float64\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Create inputs\n",
    "genres_input = create_string_input('genres_c')\n",
    "keywords_input = create_string_input('keywords_c')\n",
    "overview_input = create_string_input('overview')\n",
    "producers_input = create_string_input('producers_c')\n",
    "countries_input = create_string_input('countries_c')\n",
    "languages_input = create_string_input('languages_c')\n",
    "tagline_input = create_string_input('tagline')\n",
    "scalar_input = create_numeric_input('scalars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Create individual neural network architectures\n",
    "# Genres\n",
    "genres_features = vectorized_genres(genres_input)\n",
    "genres_features = layers.Embedding(2000, 3, name='embedding_genres')(genres_features)\n",
    "genres_features = layers.Dropout(0.2)(genres_features)\n",
    "genres_features = layers.GlobalAveragePooling1D()(genres_features)\n",
    "genres_features = layers.Dropout(0.2)(genres_features)\n",
    "genres_features = layers.Dense(32, activation='sigmoid')(genres_features)\n",
    "# Keywords\n",
    "keywords_features = vectorized_genres(keywords_input)\n",
    "keywords_features = layers.Embedding(2000, 3, name='embedding_keywords')(keywords_features)\n",
    "keywords_features = layers.Dropout(0.2)(keywords_features)\n",
    "keywords_features = layers.GlobalAveragePooling1D()(keywords_features)\n",
    "keywords_features = layers.Dropout(0.2)(keywords_features)\n",
    "keywords_features = layers.Dense(32, activation='sigmoid')(keywords_features)\n",
    "# Overview\n",
    "overview_features = vectorized_genres(overview_input)\n",
    "overview_features = layers.Embedding(2000, 3, name='embedding_overview')(overview_features)\n",
    "overview_features = layers.Dropout(0.2)(overview_features)\n",
    "overview_features = layers.GlobalAveragePooling1D()(overview_features)\n",
    "overview_features = layers.Dropout(0.2)(overview_features)\n",
    "overview_features = layers.Dense(32, activation='sigmoid')(overview_features)\n",
    "# Producers\n",
    "producers_features = vectorized_genres(producers_input)\n",
    "producers_features = layers.Embedding(2000, 3, name='embedding_producers')(producers_features)\n",
    "producers_features = layers.Dropout(0.2)(producers_features)\n",
    "producers_features = layers.GlobalAveragePooling1D()(producers_features)\n",
    "producers_features = layers.Dropout(0.2)(producers_features)\n",
    "producers_features = layers.Dense(32, activation='sigmoid')(producers_features)\n",
    "# Countries\n",
    "countries_features = vectorized_genres(countries_input)\n",
    "countries_features = layers.Embedding(2000, 3, name='embedding_countries')(countries_features)\n",
    "countries_features = layers.Dropout(0.2)(countries_features)\n",
    "countries_features = layers.GlobalAveragePooling1D()(countries_features)\n",
    "countries_features = layers.Dropout(0.2)(countries_features)\n",
    "countries_features = layers.Dense(32, activation='sigmoid')(countries_features)\n",
    "# Languages\n",
    "languages_features = vectorized_genres(languages_input)\n",
    "languages_features = layers.Embedding(2000, 3, name='embedding_languages')(languages_features)\n",
    "languages_features = layers.Dropout(0.2)(languages_features)\n",
    "languages_features = layers.GlobalAveragePooling1D()(languages_features)\n",
    "languages_features = layers.Dropout(0.2)(languages_features)\n",
    "languages_features = layers.Dense(32, activation='sigmoid')(languages_features)\n",
    "# Tagline\n",
    "tagline_features = vectorized_tagline(tagline_input)\n",
    "tagline_features = layers.Embedding(2000, 3, name='embedding_tagline')(tagline_features)\n",
    "tagline_features = layers.Dropout(0.2)(tagline_features)\n",
    "tagline_features = layers.GlobalAveragePooling1D()(tagline_features)\n",
    "tagline_features = layers.Dropout(0.2)(tagline_features)\n",
    "tagline_features = layers.Dense(32, activation='sigmoid')(tagline_features)\n",
    "# Scalars\n",
    "scalar_features = layers.Dense(32, activation='sigmoid')(scalar_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " genres_c (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " keywords_c (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " overview (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " producers_c (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " countries_c (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " languages_c (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " tagline (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization (TextVe  (None, 500)                  0         ['genres_c[0][0]',            \n",
      " ctorization)                                                        'keywords_c[0][0]',          \n",
      "                                                                     'overview[0][0]',            \n",
      "                                                                     'producers_c[0][0]',         \n",
      "                                                                     'countries_c[0][0]',         \n",
      "                                                                     'languages_c[0][0]']         \n",
      "                                                                                                  \n",
      " text_vectorization_6 (Text  (None, 500)                  0         ['tagline[0][0]']             \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_genres (Embeddin  (None, 500, 3)               6000      ['text_vectorization[0][0]']  \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " embedding_keywords (Embedd  (None, 500, 3)               6000      ['text_vectorization[1][0]']  \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_overview (Embedd  (None, 500, 3)               6000      ['text_vectorization[2][0]']  \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_producers (Embed  (None, 500, 3)               6000      ['text_vectorization[3][0]']  \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " embedding_countries (Embed  (None, 500, 3)               6000      ['text_vectorization[4][0]']  \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " embedding_languages (Embed  (None, 500, 3)               6000      ['text_vectorization[5][0]']  \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " embedding_tagline (Embeddi  (None, 500, 3)               6000      ['text_vectorization_6[0][0]']\n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 500, 3)               0         ['embedding_genres[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 500, 3)               0         ['embedding_keywords[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 500, 3)               0         ['embedding_overview[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 500, 3)               0         ['embedding_producers[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 500, 3)               0         ['embedding_countries[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 500, 3)               0         ['embedding_languages[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 500, 3)               0         ['embedding_tagline[0][0]']   \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 3)                    0         ['dropout[0][0]']             \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 3)                    0         ['dropout_2[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 3)                    0         ['dropout_4[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 3)                    0         ['dropout_6[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 3)                    0         ['dropout_8[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5  (None, 3)                    0         ['dropout_10[0][0]']          \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6  (None, 3)                    0         ['dropout_12[0][0]']          \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 3)                    0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 3)                    0         ['global_average_pooling1d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 3)                    0         ['global_average_pooling1d_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " scalars (InputLayer)        [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   128       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   128       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 32)                   128       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 32)                   128       ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 32)                   128       ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 32)                   128       ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 32)                   128       ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 32)                   224       ['scalars[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 256)                  0         ['dense[0][0]',               \n",
      "                                                                     'dense_1[0][0]',             \n",
      "                                                                     'dense_2[0][0]',             \n",
      "                                                                     'dense_3[0][0]',             \n",
      "                                                                     'dense_4[0][0]',             \n",
      "                                                                     'dense_5[0][0]',             \n",
      "                                                                     'dense_6[0][0]',             \n",
      "                                                                     'dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 32)                   8224      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " vote_average (Dense)        (None, 1)                    33        ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51377 (200.69 KB)\n",
      "Trainable params: 51377 (200.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3.8 Create main architecture\n",
    "main = layers.concatenate([genres_features, keywords_features, \n",
    "                           overview_features, producers_features,\n",
    "                           countries_features, languages_features,\n",
    "                           tagline_features, scalar_features])\n",
    "main = layers.Dense(32)(main)\n",
    "output = layers.Dense(1, name='vote_average', activation='sigmoid')(main)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[genres_input, keywords_input,\n",
    "            overview_input, producers_input,\n",
    "            countries_input, languages_input,\n",
    "            tagline_input, scalar_input],\n",
    "    outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "127/127 [==============================] - 22s 162ms/step - loss: 0.0107 - val_loss: 0.0081\n",
      "Epoch 2/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0086 - val_loss: 0.0081\n",
      "Epoch 3/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 4/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 5/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0081 - val_loss: 0.0077\n",
      "Epoch 6/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 7/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0078 - val_loss: 0.0073\n",
      "Epoch 8/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 9/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 10/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 11/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 12/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 13/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 14/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 15/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 16/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 17/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 18/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 19/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 20/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 21/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 22/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 23/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 24/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 25/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 26/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 27/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 28/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 29/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 30/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 31/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 32/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 33/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 34/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 35/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 36/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 37/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 38/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 39/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0074 - val_loss: 0.0071\n",
      "Epoch 40/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 41/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 42/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 43/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 44/50\n",
      "127/127 [==============================] - 19s 149ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 45/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 46/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 47/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 48/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 49/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 50/50\n",
      "127/127 [==============================] - 19s 150ms/step - loss: 0.0071 - val_loss: 0.0067\n"
     ]
    }
   ],
   "source": [
    "# 3.9 Train the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0072\n",
      "RMSE for test: 0.00720\n"
     ]
    }
   ],
   "source": [
    "# 3.10 Calculate metrics\n",
    "rmse = model.evaluate(test)\n",
    "print(f'RMSE for test: {rmse:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://github.com/PhilChodrow/PIC16B/blob/7d12d32e070e7ff3840b971c0ce4185ef1911796/discussion/tmdb.ipynb#L758"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
